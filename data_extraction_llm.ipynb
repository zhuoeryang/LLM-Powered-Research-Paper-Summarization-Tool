{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c35157d",
   "metadata": {},
   "source": [
    "<div class=\"list-group\">\n",
    "\n",
    "  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role='tab' aria-controls=\"home\"\n",
    "      style=\"background-color:DODGERBLUE; color:white\">Table of Contents</h3>\n",
    "\n",
    "  <a href='#Import Packages' style=\"background-color:white; color:grey\" class=\"list-group-item list-group-item-action\">1. Import Packages</a>\n",
    "\n",
    "  <a href='#Define LLM' style=\"background-color:white; color:grey\" class=\"list-group-item list-group-item-action\">2. Define LLM</a>\n",
    "\n",
    "  <a href='#Process PDFs' style=\"background-color:white; color:grey\" class=\"list-group-item list-group-item-action\">3. Process PDFs</a>\n",
    "  \n",
    "  <a href='#Create Embeddings' style=\"background-color:white; color:grey\" class=\"list-group-item list-group-item-action\">4. Create Embeddings</a>\n",
    "  \n",
    "  <a href='#Create Vector Database' style=\"background-color:white; color:grey\" class=\"list-group-item list-group-item-action\">5. Create Vector Database</a>\n",
    "\n",
    "  <a href='#Query Relevant information' style=\"background-color:white; color:grey\" class=\"list-group-item list-group-item-action\">6. Query Relevant Information \n",
    "  </a>\n",
    "\n",
    "  <a href='#Define Prompt Template' style=\"background-color:white; color:grey\" class=\"list-group-item list-group-item-action\">7. Define Prompt Template</a>\n",
    "\n",
    "  <a href='#Define Langchain' style=\"background-color:white; color:grey\" class=\"list-group-item list-group-item-action\">8. Define Langchain</a>\n",
    "\n",
    "  <a href='#Structure Response' style=\"background-color:white; color:grey\" class=\"list-group-item list-group-item-action\">9. Structure Response</a>\n",
    "\n",
    "  <a href='#Structure Response into Tabular Format for User Consumption' style=\"background-color:white; color:grey\" class=\"list-group-item list-group-item-action\">10. Structure Response into Tabular Format for User Consumption</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b268d",
   "metadata": {},
   "source": [
    "#### <a id='Import Packages' style=\"color:blue;font-size:140%;\"> 1. Import Packages</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f678098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Langchain modules\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.evaluation import load_evaluator\n",
    "# Other modules and packages\n",
    "import os\n",
    "import tempfile\n",
    "import streamlit as st  \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import uuid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f8fee8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read api key from .env file\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2adb3283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get API key for testing\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352be214",
   "metadata": {},
   "source": [
    "#### <a id='Define LLM' style=\"color:blue;font-size:140%;\"> 2. Define LLM</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2334a497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the dog sit in the shade?\\n\\nBecause he didn’t want to become a hot dog!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 13, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BPh8L5kcmAXlDgq9MLFos9duZv6tx', 'finish_reason': 'stop', 'logprobs': None}, id='run-e1a77553-1f5f-4d0d-b043-645c37ca6fd0-0', usage_metadata={'input_tokens': 13, 'output_tokens': 21, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#can choose model based on tasks and requirements\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "#test\n",
    "llm.invoke(\"Tell me a joke about dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923383f5",
   "metadata": {},
   "source": [
    "#### <a id='Process PDFs' style=\"color:blue;font-size:140%;\"> 3. Process PDFs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "189e43d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-08-29T01:23:59+00:00', 'author': '', 'keywords': '', 'moddate': '2017-08-29T01:23:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/NeuCF.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='Neural Collaborative Filtering∗\\nXiangnan He\\nNational University of\\nSingapore, Singapore\\nxiangnanhe@gmail.com\\nLizi Liao\\nNational University of\\nSingapore, Singapore\\nliaolizi.llz@gmail.com\\nHanwang Zhang\\nColumbia University\\nUSA\\nhanwangzhang@gmail.com\\nLiqiang Nie\\nShandong University\\nChina\\nnieliqiang@gmail.com\\nXia Hu\\nTexas A&M University\\nUSA\\nhu@cse.tamu.edu\\nTat-Seng Chua\\nNational University of\\nSingapore, Singapore\\ndcscts@nus.edu.sg\\nABSTRACT\\nIn recent years, deep neural networks have yielded immense\\nsuccess on speech recognition, computer vision and natural\\nlanguage processing. However, the exploration of deep neu-\\nral networks on recommender systems has received relatively\\nless scrutiny. In this work, we strive to develop techniques\\nbased on neural networks to tackle the key problem in rec-\\nommendation — collaborative ﬁltering — on the basis of\\nimplicit feedback.\\nAlthough some recent work has employed deep learning\\nfor recommendation, they primarily used it to model auxil-\\niary information, such as textual descriptions of items and\\nacoustic features of musics. When it comes to model the key\\nfactor in collaborative ﬁltering — the interaction between\\nuser and item features, they still resorted to matrix factor-\\nization and applied an inner product on the latent features\\nof users and items.\\nBy replacing the inner product with a neural architecture\\nthat can learn an arbitrary function from data, we present\\na general framework named NCF, short for Neural network-\\nbased Collaborative Filtering . NCF is generic and can ex-\\npress and generalize matrix factorization under its frame-\\nwork. To supercharge NCF modelling with non-linearities,\\nwe propose to leverage a multi-layer perceptron to learn the\\nuser–item interaction function. Extensive experiments on\\ntwo real-world datasets show signiﬁcant improvements of our\\nproposed NCF framework over the state-of-the-art methods.\\nEmpirical evidence shows that using deeper layers of neural\\nnetworks oﬀers better recommendation performance.\\nKeywords\\nCollaborative Filtering, Neural Networks, Deep Learning,\\nMatrix Factorization, Implicit Feedback\\n∗NExT research is supported by the National Research\\nFoundation, Prime Minister’s Oﬃce, Singapore under its\\nIRC@SG Funding Initiative.\\nc⃝2017 International World Wide Web Conference Committee\\n(IW3C2), published under Creative Commons CC BY 4.0 License.\\nWWW 2017,April 3–7, 2017, Perth, Australia.\\nACM 978-1-4503-4913-0/17/04.\\nhttp://dx.doi.org/10.1145/3038912.3052569\\n.\\n1. INTRODUCTION\\nIn the era of information explosion, recommender systems\\nplay a pivotal role in alleviating information overload, hav-\\ning been widely adopted by many online services, including\\nE-commerce, online news and social media sites. The key to\\na personalized recommender system is in modelling users’\\npreference on items based on their past interactions ( e.g.,\\nratings and clicks), known as collaborative ﬁltering [31, 46].\\nAmong the various collaborative ﬁltering techniques, matrix\\nfactorization (MF) [14, 21] is the most popular one, which\\nprojects users and items into a shared latent space, using\\na vector of latent features to represent a user or an item.\\nThereafter a user’s interaction on an item is modelled as the\\ninner product of their latent vectors.\\nPopularized by the Netﬂix Prize, MF has become the de\\nfacto approach to latent factor model-based recommenda-\\ntion. Much research eﬀort has been devoted to enhancing\\nMF, such as integrating it with neighbor-based models [21],\\ncombining it with topic models of item content [38], and ex-\\ntending it to factorization machines [26] for a generic mod-\\nelling of features. Despite the eﬀectiveness of MF for collab-\\norative ﬁltering, it is well-known that its performance can be\\nhindered by the simple choice of the interaction function —\\ninner product. For example, for the task of rating prediction\\non explicit feedback, it is well known that the performance\\nof the MF model can be improved by incorporating user\\nand item bias terms into the interaction function 1. While\\nit seems to be just a trivial tweak for the inner product\\noperator [14], it points to the positive eﬀect of designing a\\nbetter, dedicated interaction function for modelling the la-\\ntent feature interactions between users and items. The inner\\nproduct, which simply combines the multiplication of latent\\nfeatures linearly, may not be suﬃcient to capture the com-\\nplex structure of user interaction data.\\nThis paper explores the use of deep neural networks for\\nlearning the interaction function from data, rather than a\\nhandcraft that has been done by many previous work [18,\\n21]. The neural network has been proven to be capable of\\napproximating any continuous function [17], and more re-\\ncently deep neural networks (DNNs) have been found to be\\neﬀective in several domains, ranging from computer vision,\\nspeech recognition, to text processing [5, 10, 15, 47]. How-\\never, there is relatively little work on employing DNNs for\\nrecommendation in contrast to the vast amount of literature\\n1http://alex.smola.org/teaching/berkeley2012/slides/8_\\nRecommender.pdf\\narXiv:1708.05031v2  [cs.IR]  26 Aug 2017'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-08-29T01:23:59+00:00', 'author': '', 'keywords': '', 'moddate': '2017-08-29T01:23:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/NeuCF.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content=\"on MF methods. Although some recent advances [37, 38,\\n45] have applied DNNs to recommendation tasks and shown\\npromising results, they mostly used DNNs to model auxil-\\niary information, such as textual description of items, audio\\nfeatures of musics, and visual content of images. With re-\\ngards to modelling the key collaborative ﬁltering eﬀect, they\\nstill resorted to MF, combining user and item latent features\\nusing an inner product.\\nThis work addresses the aforementioned research prob-\\nlems by formalizing a neural network modelling approach for\\ncollaborative ﬁltering. We focus on implicit feedback, which\\nindirectly reﬂects users’ preference through behaviours like\\nwatching videos, purchasing products and clicking items.\\nCompared to explicit feedback ( i.e., ratings and reviews),\\nimplicit feedback can be tracked automatically and is thus\\nmuch easier to collect for content providers. However, it is\\nmore challenging to utilize, since user satisfaction is not ob-\\nserved and there is a natural scarcity of negative feedback.\\nIn this paper, we explore the central theme of how to utilize\\nDNNs to model noisy implicit feedback signals.\\nThe main contributions of this work are as follows.\\n1. We present a neural network architecture to model\\nlatent features of users and items and devise a gen-\\neral framework NCF for collaborative ﬁltering based\\non neural networks.\\n2. We show that MF can be interpreted as a specialization\\nof NCF and utilize a multi-layer perceptron to endow\\nNCF modelling with a high level of non-linearities.\\n3. We perform extensive experiments on two real-world\\ndatasets to demonstrate the eﬀectiveness of our NCF\\napproaches and the promise of deep learning for col-\\nlaborative ﬁltering.\\n2. PRELIMINARIES\\nWe ﬁrst formalize the problem and discuss existing solu-\\ntions for collaborative ﬁltering with implicit feedback. We\\nthen shortly recapitulate the widely used MF model, high-\\nlighting its limitation caused by using an inner product.\\n2.1 Learning from Implicit Data\\nLet M and N denote the number of users and items,\\nrespectively. We deﬁne the user–item interaction matrix\\nY ∈RM×N from users’ implicit feedback as,\\nyui =\\n{\\n1, if interaction (user u, item i) is observed;\\n0, otherwise. (1)\\nHere a value of 1 for yui indicates that there is an interac-\\ntion between user uand item i; however, it does not mean u\\nactually likes i. Similarly, a value of 0 does not necessarily\\nmean u does not like i, it can be that the user is not aware\\nof the item. This poses challenges in learning from implicit\\ndata, since it provides only noisy signals about users’ pref-\\nerence. While observed entries at least reﬂect users’ interest\\non items, the unobserved entries can be just missing data\\nand there is a natural scarcity of negative feedback.\\nThe recommendation problem with implicit feedback is\\nformulated as the problem of estimating the scores of unob-\\nserved entries in Y, which are used for ranking the items.\\nModel-based approaches assume that data can be generated\\n(or described) by an underlying model. Formally, they can\\nbe abstracted as learning ˆyui = f(u,i|Θ),where ˆyui denotes\\nu1\\nu2\\nu3\\nu4\\ni1 i2 i3 i4 i5\\n1 1 1 0 1\\n0 1 1 0 0\\n0 1 1 1 0\\n1 0 1 1 1\\nitems \\nusers \\n(a) user–item matrix\\np1\\np2\\np3\\np4\\np' 4 (b) user latent space\\nFigure 1: An example illustrates MF’s limitation.\\nFrom data matrix (a), u4 is most similar to u1, fol-\\nlowed by u3, and lastly u2. However in the latent\\nspace (b), placing p4 closest to p1 makes p4 closer to\\np2 than p3, incurring a large ranking loss.\\nthe predicted score of interaction yui, Θ denotes model pa-\\nrameters, and f denotes the function that maps model pa-\\nrameters to the predicted score (which we term as an inter-\\naction function ).\\nTo estimate parameters Θ, existing approaches generally\\nfollow the machine learning paradigm that optimizes an ob-\\njective function. Two types of objective functions are most\\ncommonly used in literature — pointwise loss [14, 19] and\\npairwise loss [27, 33]. As a natural extension of abundant\\nwork on explicit feedback [21, 46], methods on pointwise\\nlearning usually follow a regression framework by minimiz-\\ning the squared loss between ˆ yui and its target value yui.\\nTo handle the absence of negative data, they have either\\ntreated all unobserved entries as negative feedback, or sam-\\npled negative instances from unobserved entries [14]. For\\npairwise learning [27, 44], the idea is that observed entries\\nshould be ranked higher than the unobserved ones. As such,\\ninstead of minimizing the loss between ˆyui and yui, pairwise\\nlearning maximizes the margin between observed entry ˆyui\\nand unobserved entry ˆyuj.\\nMoving one step forward, our NCF framework parame-\\nterizes the interaction function f using neural networks to\\nestimate ˆyui. As such, it naturally supports both pointwise\\nand pairwise learning.\\n2.2 Matrix Factorization\\nMF associates each user and item with a real-valued vector\\nof latent features. Let pu and qi denote the latent vector for\\nuser uand item i, respectively; MF estimates an interaction\\nyui as the inner product of pu and qi:\\nˆyui = f(u,i|pu,qi) = pT\\nu qi =\\nK∑\\nk=1\\npukqik, (2)\\nwhere K denotes the dimension of the latent space. As we\\ncan see, MF models the two-way interaction of user and item\\nlatent factors, assuming each dimension of the latent space\\nis independent of each other and linearly combining them\\nwith the same weight. As such, MF can be deemed as a\\nlinear model of latent factors.\\nFigure 1 illustrates how the inner product function can\\nlimit the expressiveness of MF. There are two settings to be\\nstated clearly beforehand to understand the example well.\\nFirst, since MF maps users and items to the same latent\\nspace, the similarity between two users can also be measured\\nwith an inner product, or equivalently 2, the cosine of the\\nangle between their latent vectors. Second, without loss of\\n2Assuming latent vectors are of a unit length.\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-08-29T01:23:59+00:00', 'author': '', 'keywords': '', 'moddate': '2017-08-29T01:23:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/NeuCF.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3'}, page_content='Input Layer (Sparse)\\nEmbedding Layer\\nNeural CF Layers\\nOutput Layer\\n1000 0 0 ……\\nUser (u)\\n0000 1 0 ……\\nItem (i)\\nUser Latent Vector Item Latent Vector\\nLayer 1\\nLayer 2\\nLayer X\\n……\\nScore TargetTrainingŷui yui\\nPM×K = {puk} QN×K = {qik}\\nFigure 2: Neural collaborative ﬁltering framework\\ngenerality, we use the Jaccard coeﬃcient 3 as the ground-\\ntruth similarity of two users that MF needs to recover.\\nLet us ﬁrst focus on the ﬁrst three rows (users) in Fig-\\nure 1a. It is easy to have s23(0.66) > s12(0.5) > s13(0.4).\\nAs such, the geometric relations of p1,p2,and p3 in the la-\\ntent space can be plotted as in Figure 1b. Now, let us con-\\nsider a new user u4, whose input is given as the dashed line\\nin Figure 1a. We can have s41(0.6) > s43(0.4) > s42(0.2),\\nmeaning that u4 is most similar to u1, followed by u3, and\\nlastly u2. However, if a MF model places p4 closest to p1\\n(the two options are shown in Figure 1b with dashed lines),\\nit will result in p4 closer to p2 than p3, which unfortunately\\nwill incur a large ranking loss.\\nThe above example shows the possible limitation of MF\\ncaused by the use of a simple and ﬁxed inner product to esti-\\nmate complex user–item interactions in the low-dimensional\\nlatent space. We note that one way to resolve the issue is\\nto use a large number of latent factors K. However, it may\\nadversely hurt the generalization of the model ( e.g., over-\\nﬁtting the data), especially in sparse settings [26]. In this\\nwork, we address the limitation by learning the interaction\\nfunction using DNNs from data.\\n3. NEURAL COLLABORATIVE FILTERING\\nWe ﬁrst present the general NCF framework, elaborat-\\ning how to learn NCF with a probabilistic model that em-\\nphasizes the binary property of implicit data. We then\\nshow that MF can be expressed and generalized under NCF.\\nTo explore DNNs for collaborative ﬁltering, we then pro-\\npose an instantiation of NCF, using a multi-layer perceptron\\n(MLP) to learn the user–item interaction function. Lastly,\\nwe present a new neural matrix factorization model, which\\nensembles MF and MLP under the NCF framework; it uni-\\nﬁes the strengths of linearity of MF and non-linearity of\\nMLP for modelling the user–item latent structures.\\n3.1 General Framework\\nTo permit a full neural treatment of collaborative ﬁltering,\\nwe adopt a multi-layer representation to model a user–item\\ninteraction yui as shown in Figure 2, where the output of one\\nlayer serves as the input of the next one. The bottom input\\nlayer consists of two feature vectorsvU\\nu and vI\\ni that describe\\nuser u and item i, respectively; they can be customized to\\nsupport a wide range of modelling of users and items, such\\n3Let Ru be the set of items that user uhas interacted with,\\nthen the Jaccard similarity of users i and j is deﬁned as\\nsij =\\n|Ri |∩|Rj |\\n|Ri |∪|Rj |.\\nas context-aware [28, 1], content-based [3], and neighbor-\\nbased [26]. Since this work focuses on the pure collaborative\\nﬁltering setting, we use only the identity of a user and an\\nitem as the input feature, transforming it to a binarized\\nsparse vector with one-hot encoding. Note that with such a\\ngeneric feature representation for inputs, our method can be\\neasily adjusted to address the cold-start problem by using\\ncontent features to represent users and items.\\nAbove the input layer is the embedding layer; it is a fully\\nconnected layer that projects the sparse representation to\\na dense vector. The obtained user (item) embedding can\\nbe seen as the latent vector for user (item) in the context\\nof latent factor model. The user embedding and item em-\\nbedding are then fed into a multi-layer neural architecture,\\nwhich we term asneural collaborative ﬁltering layers, to map\\nthe latent vectors to prediction scores. Each layer of the neu-\\nral CF layers can be customized to discover certain latent\\nstructures of user–item interactions. The dimension of the\\nlast hidden layer X determines the model’s capability. The\\nﬁnal output layer is the predicted score ˆ yui, and training\\nis performed by minimizing the pointwise loss between ˆ yui\\nand its target value yui. We note that another way to train\\nthe model is by performing pairwise learning, such as using\\nthe Bayesian Personalized Ranking [27] and margin-based\\nloss [33]. As the focus of the paper is on the neural network\\nmodelling part, we leave the extension to pairwise learning\\nof NCF as a future work.\\nWe now formulate the NCF’s predictive model as\\nˆyui = f(PT vU\\nu ,QT vI\\ni |P,Q,Θf ), (3)\\nwhere P ∈RM×K and Q ∈RN×K, denoting the latent fac-\\ntor matrix for users and items, respectively; and Θf denotes\\nthe model parameters of the interaction function f. Since\\nthe function f is deﬁned as a multi-layer neural network, it\\ncan be formulated as\\nf(PT vU\\nu ,QT vI\\ni ) = φout(φX(...φ2(φ1(PT vU\\nu ,QT vI\\ni ))...)),\\n(4)\\nwhere φout and φx respectively denote the mapping function\\nfor the output layer and x-th neural collaborative ﬁltering\\n(CF) layer, and there are X neural CF layers in total.\\n3.1.1 Learning NCF\\nTo learn model parameters, existing pointwise methods [14,\\n39] largely perform a regression with squared loss:\\nLsqr =\\n∑\\n(u,i)∈Y∪Y−\\nwui(yui −ˆyui)2, (5)\\nwhere Ydenotes the set of observed interactions in Y, and\\nY−denotes the set of negative instances, which can be all (or\\nsampled from) unobserved interactions; and wui is a hyper-\\nparameter denoting the weight of training instance ( u,i).\\nWhile the squared loss can be explained by assuming that\\nobservations are generated from a Gaussian distribution [29],\\nwe point out that it may not tally well with implicit data.\\nThis is because for implicit data, the target value yui is\\na binarized 1 or 0 denoting whether u has interacted with\\ni. In what follows, we present a probabilistic approach for\\nlearning the pointwise NCF that pays special attention to\\nthe binary property of implicit data.\\nConsidering the one-class nature of implicit feedback, we\\ncan view the value of yui as a label — 1 means item i is\\nrelevant to u, and 0 otherwise. The prediction score ˆ yui'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-08-29T01:23:59+00:00', 'author': '', 'keywords': '', 'moddate': '2017-08-29T01:23:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/NeuCF.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4'}, page_content='then represents how likely iis relevant to u. To endow NCF\\nwith such a probabilistic explanation, we need to constrain\\nthe output ˆyui in the range of [0 ,1], which can be easily\\nachieved by using a probabilistic function (e.g., the Logistic\\nor Probit function) as the activation function for the output\\nlayer φout. With the above settings, we then deﬁne the\\nlikelihood function as\\np(Y,Y−|P,Q,Θf ) =\\n∏\\n(u,i)∈Y\\nˆyui\\n∏\\n(u,j)∈Y−\\n(1 −ˆyuj). (6)\\nTaking the negative logarithm of the likelihood, we reach\\nL= −\\n∑\\n(u,i)∈Y\\nlog ˆyui −\\n∑\\n(u,j)∈Y−\\nlog(1 −ˆyuj)\\n= −\\n∑\\n(u,i)∈Y∪Y−\\nyui log ˆyui + (1 −yui) log(1−ˆyui).\\n(7)\\nThis is the objective function to minimize for the NCF meth-\\nods, and its optimization can be done by performing stochas-\\ntic gradient descent (SGD). Careful readers might have real-\\nized that it is the same as the binary cross-entropy loss, also\\nknown as log loss. By employing a probabilistic treatment\\nfor NCF, we address recommendation with implicit feedback\\nas a binary classiﬁcation problem. As the classiﬁcation-\\naware log loss has rarely been investigated in recommen-\\ndation literature, we explore it in this work and empirically\\nshow its eﬀectiveness in Section 4.3. For the negative in-\\nstances Y−, we uniformly sample them from unobserved in-\\nteractions in each iteration and control the sampling ratio\\nw.r.t. the number of observed interactions. While a non-\\nuniform sampling strategy (e.g., item popularity-biased [14,\\n12]) might further improve the performance, we leave the\\nexploration as a future work.\\n3.2 Generalized Matrix Factorization (GMF)\\nWe now show how MF can be interpreted as a special case\\nof our NCF framework. As MF is the most popular model\\nfor recommendation and has been investigated extensively\\nin literature, being able to recover it allows NCF to mimic\\na large family of factorization models [26].\\nDue to the one-hot encoding of user (item) ID of the input\\nlayer, the obtained embedding vector can be seen as the\\nlatent vector of user (item). Let the user latent vector pu\\nbe PT vU\\nu and item latent vector qi be QT vI\\ni . We deﬁne the\\nmapping function of the ﬁrst neural CF layer as\\nφ1(pu,qi) = pu ⊙qi, (8)\\nwhere ⊙denotes the element-wise product of vectors. We\\nthen project the vector to the output layer:\\nˆyui = aout(hT (pu ⊙qi)), (9)\\nwhere aout and h denote the activation function and edge\\nweights of the output layer, respectively. Intuitively, if we\\nuse an identity function for aout and enforce h to be a uni-\\nform vector of 1, we can exactly recover the MF model.\\nUnder the NCF framework, MF can be easily general-\\nized and extended. For example, if we allow h to be learnt\\nfrom data without the uniform constraint, it will result in\\na variant of MF that allows varying importance of latent\\ndimensions. And if we use a non-linear function for aout, it\\nwill generalize MF to a non-linear setting which might be\\nmore expressive than the linear MF model. In this work, we\\nimplement a generalized version of MF under NCF that uses\\nthe sigmoid function σ(x) = 1/(1 + e−x) as aout and learns\\nh from data with the log loss (Section 3.1.1). We term it as\\nGMF, short for Generalized Matrix Factorization.\\n3.3 Multi-Layer Perceptron (MLP)\\nSince NCF adopts two pathways to model users and items,\\nit is intuitive to combine the features of two pathways by\\nconcatenating them. This design has been widely adopted\\nin multimodal deep learning work [47, 34]. However, simply\\na vector concatenation does not account for any interactions\\nbetween user and item latent features, which is insuﬃcient\\nfor modelling the collaborative ﬁltering eﬀect. To address\\nthis issue, we propose to add hidden layers on the concate-\\nnated vector, using a standard MLP to learn the interaction\\nbetween user and item latent features. In this sense, we can\\nendow the model a large level of ﬂexibility and non-linearity\\nto learn the interactions between pu and qi, rather than the\\nway of GMF that uses only a ﬁxed element-wise product\\non them. More precisely, the MLP model under our NCF\\nframework is deﬁned as\\nz1 = φ1(pu,qi) =\\n[\\npu\\nqi\\n]\\n,\\nφ2(z1) = a2(WT\\n2 z1 + b2),\\n......\\nφL(zL−1) = aL(WT\\nLzL−1 + bL),\\nˆyui = σ(hT φL(zL−1)),\\n(10)\\nwhere Wx, bx, and ax denote the weight matrix, bias vec-\\ntor, and activation function for the x-th layer’s perceptron,\\nrespectively. For activation functions of MLP layers, one\\ncan freely choose sigmoid, hyperbolic tangent (tanh), and\\nRectiﬁer (ReLU), among others. We would like to ana-\\nlyze each function: 1) The sigmoid function restricts each\\nneuron to be in (0,1), which may limit the model’s perfor-\\nmance; and it is known to suﬀer from saturation, where\\nneurons stop learning when their output is near either 0 or\\n1. 2) Even though tanh is a better choice and has been\\nwidely adopted [6, 44], it only alleviates the issues of sig-\\nmoid to a certain extent, since it can be seen as a rescaled\\nversion of sigmoid (tanh( x/2) = 2 σ(x) −1). And 3) as\\nsuch, we opt for ReLU, which is more biologically plausi-\\nble and proven to be non-saturated [9]; moreover, it encour-\\nages sparse activations, being well-suited for sparse data and\\nmaking the model less likely to be overﬁtting. Our empirical\\nresults show that ReLU yields slightly better performance\\nthan tanh, which in turn is signiﬁcantly better than sigmoid.\\nAs for the design of network structure, a common solution\\nis to follow a tower pattern, where the bottom layer is the\\nwidest and each successive layer has a smaller number of\\nneurons (as in Figure 2). The premise is that by using a\\nsmall number of hidden units for higher layers, they can\\nlearn more abstractive features of data [10]. We empirically\\nimplement the tower structure, halving the layer size for\\neach successive higher layer.\\n3.4 Fusion of GMF and MLP\\nSo far we have developed two instantiations of NCF —\\nGMF that applies a linear kernel to model the latent feature\\ninteractions, and MLP that uses a non-linear kernel to learn\\nthe interaction function from data. The question then arises:\\nhow can we fuse GMF and MLP under the NCF framework,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-08-29T01:23:59+00:00', 'author': '', 'keywords': '', 'moddate': '2017-08-29T01:23:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/NeuCF.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5'}, page_content='1000 0 0 ……\\nUser (u)\\n0000 1 0 ……\\nItem ( i )\\nMF User Vector MF Item Vector\\nGMF Layer ……\\nScore TargetTrainingŷui yui\\nMLP Layer 1\\nMLP User Vector MLP Item Vector\\nElement-wise \\nProduct\\nConcatenation\\nMLP Layer 2\\nMLP Layer X\\nNeuMF Layer\\nLog loss\\n𝝈\\nReLU\\nReLU\\nConcatenation\\nFigure 3: Neural matrix factorization model\\nso that they can mutually reinforce each other to better\\nmodel the complex user-iterm interactions?\\nA straightforward solution is to let GMF and MLP share\\nthe same embedding layer, and then combine the outputs of\\ntheir interaction functions. This way shares a similar spirit\\nwith the well-known Neural Tensor Network (NTN) [33].\\nSpeciﬁcally, the model for combining GMF with a one-layer\\nMLP can be formulated as\\nˆyui = σ(hT a(pu ⊙qi + W\\n[\\npu\\nqi\\n]\\n+ b)). (11)\\nHowever, sharing embeddings of GMF and MLP might\\nlimit the performance of the fused model. For example,\\nit implies that GMF and MLP must use the same size of\\nembeddings; for datasets where the optimal embedding size\\nof the two models varies a lot, this solution may fail to obtain\\nthe optimal ensemble.\\nTo provide more ﬂexibility to the fused model, we allow\\nGMF and MLP to learn separate embeddings, and combine\\nthe two models by concatenating their last hidden layer.\\nFigure 3 illustrates our proposal, the formulation of which\\nis given as follows\\nφGMF = pG\\nu ⊙qG\\ni ,\\nφMLP = aL(WT\\nL(aL−1(...a2(WT\\n2\\n[\\npM\\nu\\nqM\\ni\\n]\\n+ b2)...)) + bL),\\nˆyui = σ(hT\\n[\\nφGMF\\nφMLP\\n]\\n),\\n(12)\\nwhere pG\\nu and pM\\nu denote the user embedding for GMF\\nand MLP parts, respectively; and similar notations of qG\\ni\\nand qM\\ni for item embeddings. As discussed before, we use\\nReLU as the activation function of MLP layers. This model\\ncombines the linearity of MF and non-linearity of DNNs for\\nmodelling user–item latent structures. We dub this model\\n“NeuMF”, short forNeural Matrix Factorization. The deriva-\\ntive of the model w.r.t. each model parameter can be cal-\\nculated with standard back-propagation, which is omitted\\nhere due to space limitation.\\n3.4.1 Pre-training\\nDue to the non-convexity of the objective function of NeuMF,\\ngradient-based optimization methods only ﬁnd locally-optimal\\nsolutions. It is reported that the initialization plays an im-\\nportant role for the convergence and performance of deep\\nlearning models [7]. Since NeuMF is an ensemble of GMF\\nand MLP, we propose to initialize NeuMF using the pre-\\ntrained models of GMF and MLP.\\nWe ﬁrst train GMF and MLP with random initializations\\nuntil convergence. We then use their model parameters as\\nthe initialization for the corresponding parts of NeuMF’s\\nparameters. The only tweak is on the output layer, where\\nwe concatenate weights of the two models with\\nh ←\\n[\\nαhGMF\\n(1 −α)hMLP\\n]\\n, (13)\\nwhere hGMF and hMLP denote the h vector of the pre-\\ntrained GMF and MLP model, respectively; and α is a\\nhyper-parameter determining the trade-oﬀ between the two\\npre-trained models.\\nFor training GMF and MLP from scratch, we adopt the\\nAdaptive Moment Estimation (Adam) [20], which adapts\\nthe learning rate for each parameter by performing smaller\\nupdates for frequent and larger updates for infrequent pa-\\nrameters. The Adam method yields faster convergence for\\nboth models than the vanilla SGD and relieves the pain of\\ntuning the learning rate. After feeding pre-trained parame-\\nters into NeuMF, we optimize it with the vanilla SGD, rather\\nthan Adam. This is because Adam needs to save momentum\\ninformation for updating parameters properly. As we ini-\\ntialize NeuMF with pre-trained model parameters only and\\nforgo saving the momentum information, it is unsuitable to\\nfurther optimize NeuMF with momentum-based methods.\\n4. EXPERIMENTS\\nIn this section, we conduct experiments with the aim of\\nanswering the following research questions:\\nRQ1 Do our proposed NCF methods outperform the state-\\nof-the-art implicit collaborative ﬁltering methods?\\nRQ2 How does our proposed optimization framework (log\\nloss with negative sampling) work for the recommen-\\ndation task?\\nRQ3 Are deeper layers of hidden units helpful for learning\\nfrom user–item interaction data?\\nIn what follows, we ﬁrst present the experimental settings,\\nfollowed by answering the above three research questions.\\n4.1 Experimental Settings\\nDatasets. We experimented with two publicly accessible\\ndatasets: MovieLens4 and Pinterest5. The characteristics of\\nthe two datasets are summarized in Table 1.\\n1. MovieLens . This movie rating dataset has been\\nwidely used to evaluate collaborative ﬁltering algorithms.\\nWe used the version containing one million ratings, where\\neach user has at least 20 ratings. While it is an explicit\\nfeedback data, we have intentionally chosen it to investigate\\nthe performance of learning from the implicit signal [21] of\\nexplicit feedback. To this end, we transformed it into im-\\nplicit data, where each entry is marked as 0 or 1 indicating\\nwhether the user has rated the item.\\n2. Pinterest. This implicit feedback data is constructed\\nby [8] for evaluating content-based image recommendation.\\n4http://grouplens.org/datasets/movielens/1m/\\n5https://sites.google.com/site/xueatalphabeta/\\nacademic-projects'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-08-29T01:23:59+00:00', 'author': '', 'keywords': '', 'moddate': '2017-08-29T01:23:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/NeuCF.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6'}, page_content='Table 1: Statistics of the evaluation datasets.\\nDataset Interaction# Item# User# Sparsity\\nMovieLens 1,000,209 3,706 6,040 95.53%\\nPinterest 1,500,809 9,916 55,187 99.73%\\nThe original data is very large but highly sparse. For exam-\\nple, over 20% of users have only one pin, making it diﬃcult\\nto evaluate collaborative ﬁltering algorithms. As such, we\\nﬁltered the dataset in the same way as the MovieLens data\\nthat retained only users with at least 20 interactions (pins).\\nThis results in a subset of the data that contains 55 ,187\\nusers and 1 ,500,809 interactions. Each interaction denotes\\nwhether the user has pinned the image to her own board.\\nEvaluation Protocols. To evaluate the performance of\\nitem recommendation, we adopted the leave-one-out evalu-\\nation, which has been widely used in literature [1, 14, 27].\\nFor each user, we held-out her latest interaction as the test\\nset and utilized the remaining data for training. Since it is\\ntoo time-consuming to rank all items for every user during\\nevaluation, we followed the common strategy [6, 21] that\\nrandomly samples 100 items that are not interacted by the\\nuser, ranking the test item among the 100 items. The perfor-\\nmance of a ranked list is judged by Hit Ratio (HR) and Nor-\\nmalized Discounted Cumulative Gain (NDCG) [11]. With-\\nout special mention, we truncated the ranked list at 10 for\\nboth metrics. As such, the HR intuitively measures whether\\nthe test item is present on the top-10 list, and the NDCG\\naccounts for the position of the hit by assigning higher scores\\nto hits at top ranks. We calculated both metrics for each\\ntest user and reported the average score.\\nBaselines. We compared our proposed NCF methods (GMF,\\nMLP and NeuMF) with the following methods:\\n- ItemPop. Items are ranked by their popularity judged\\nby the number of interactions. This is a non-personalized\\nmethod to benchmark the recommendation performance [27].\\n- ItemKNN [31]. This is the standard item-based col-\\nlaborative ﬁltering method. We followed the setting of [19]\\nto adapt it for implicit data.\\n- BPR [27]. This method optimizes the MF model of\\nEquation 2 with a pairwise ranking loss, which is tailored\\nto learn from implicit feedback. It is a highly competitive\\nbaseline for item recommendation. We used a ﬁxed learning\\nrate, varying it and reporting the best performance.\\n- eALS [14]. This is a state-of-the-art MF method for\\nitem recommendation. It optimizes the squared loss of Equa-\\ntion 5, treating all unobserved interactions as negative in-\\nstances and weighting them non-uniformly by the item pop-\\nularity. Since eALS shows superior performance over the\\nuniform-weighting method WMF [19], we do not further re-\\nport WMF’s performance.\\nAs our proposed methods aim to model the relationship\\nbetween users and items, we mainly compare with user–\\nitem models. We leave out the comparison with item–item\\nmodels, such as SLIM [25] and CDAE [44], because the per-\\nformance diﬀerence may be caused by the user models for\\npersonalization (as they are item–item model).\\nParameter Settings. We implemented our proposed meth-\\nods based on Keras 6. To determine hyper-parameters of\\nNCF methods, we randomly sampled one interaction for\\n6https://github.com/hexiangnan/neural_\\ncollaborative_filtering\\neach user as the validation data and tuned hyper-parameters\\non it. All NCF models are learnt by optimizing the log loss\\nof Equation 7, where we sampled four negative instances\\nper positive instance. For NCF models that are trained\\nfrom scratch, we randomly initialized model parameters with\\na Gaussian distribution (with a mean of 0 and standard\\ndeviation of 0 .01), optimizing the model with mini-batch\\nAdam [20]. We tested the batch size of [128 ,256,512,1024],\\nand the learning rate of [0.0001 ,0.0005, 0.001, 0.005]. Since\\nthe last hidden layer of NCF determines the model capa-\\nbility, we term it as predictive factors and evaluated the\\nfactors of [8,16,32,64]. It is worth noting that large factors\\nmay cause overﬁtting and degrade the performance. With-\\nout special mention, we employed three hidden layers for\\nMLP; for example, if the size of predictive factors is 8, then\\nthe architecture of the neural CF layers is 32→16 →8, and\\nthe embedding size is 16. For the NeuMF with pre-training,\\nαwas set to 0.5, allowing the pre-trained GMF and MLP to\\ncontribute equally to NeuMF’s initialization.\\n4.2 Performance Comparison (RQ1)\\nFigure 4 shows the performance of HR@10 and NDCG@10\\nwith respect to the number of predictive factors. For MF\\nmethods BPR and eALS, the number of predictive factors\\nis equal to the number of latent factors. For ItemKNN, we\\ntested diﬀerent neighbor sizes and reported the best per-\\nformance. Due to the weak performance of ItemPop, it is\\nomitted in Figure 4 to better highlight the performance dif-\\nference of personalized methods.\\nFirst, we can see that NeuMF achieves the best perfor-\\nmance on both datasets, signiﬁcantly outperforming the state-\\nof-the-art methods eALS and BPR by a large margin (on\\naverage, the relative improvement over eALS and BPR is\\n4.5% and 4 .9%, respectively). For Pinterest, even with a\\nsmall predictive factor of 8, NeuMF substantially outper-\\nforms that of eALS and BPR with a large factor of 64. This\\nindicates the high expressiveness of NeuMF by fusing the\\nlinear MF and non-linear MLP models. Second, the other\\ntwo NCF methods — GMF and MLP — also show quite\\nstrong performance. Between them, MLP slightly under-\\nperforms GMF. Note that MLP can be further improved by\\nadding more hidden layers (see Section 4.4), and here we\\nonly show the performance of three layers. For small pre-\\ndictive factors, GMF outperforms eALS on both datasets;\\nalthough GMF suﬀers from overﬁtting for large factors, its\\nbest performance obtained is better than (or on par with)\\nthat of eALS. Lastly, GMF shows consistent improvements\\nover BPR, admitting the eﬀectiveness of the classiﬁcation-\\naware log loss for the recommendation task, since GMF and\\nBPR learn the same MF model but with diﬀerent objective\\nfunctions.\\nFigure 5 shows the performance of Top- K recommended\\nlists where the ranking position K ranges from 1 to 10. To\\nmake the ﬁgure more clear, we show the performance of\\nNeuMF rather than all three NCF methods. As can be\\nseen, NeuMF demonstrates consistent improvements over\\nother methods across positions, and we further conducted\\none-sample paired t-tests, verifying that all improvements\\nare statistically signiﬁcant for p< 0.01. For baseline meth-\\nods, eALS outperforms BPR on MovieLens with about 5.1%\\nrelative improvement, while underperforms BPR on Pinter-\\nest in terms of NDCG. This is consistent with [14]’s ﬁnding\\nthat BPR can be a strong performer for ranking performance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-08-29T01:23:59+00:00', 'author': '', 'keywords': '', 'moddate': '2017-08-29T01:23:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/NeuCF.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7'}, page_content='0.55 \\n0.6 \\n0.65 \\n0.7 \\n0.75 \\n8 16 32 64 \\nHR@10 \\nFactors \\nMovieLens \\nItemKNN BPR \\neALS GMF \\nMLP NeuMF \\n(a) MovieLens — HR@10\\n0.3 \\n0.34 \\n0.38 \\n0.42 \\n0.46 \\n8 16 32 64 \\nNDCG@10 \\nFactors \\nMovieLens \\nItemKNN BPR \\neALS GMF \\nMLP NeuMF (b) MovieLens — NDCG@10\\n0.78 \\n0.81 \\n0.84 \\n0.87 \\n0.9 \\n8 16 32 64 \\nHR@10 \\nFactors \\nPinterest \\nItemKNN BPR \\neALS GMF \\nMLP NeuMF (c) Pinterest — HR@10\\n0.48 \\n0.5 \\n0.52 \\n0.54 \\n0.56 \\n8 16 32 64 \\nNDCG@10 \\nFactors \\nPinterest \\nItemKNN BPR \\neALS GMF \\nMLP NeuMF (d) Pinterest — NDCG@10\\nFigure 4: Performance of HR@10 and NDCG@10 w.r.t. the number of predictive factors on the two datasets.\\n0.1 \\n0.25 \\n0.4 \\n0.55 \\n0.7 \\n1 2 3 4 5 6 7 8 9 10 \\nHR@K \\nK\\nMovieLens \\nItemPop ItemKNN \\nBPR eALS \\nNeuMF \\n(a) MovieLens — HR@K\\n0.1 \\n0.18 \\n0.26 \\n0.34 \\n0.42 \\n1 2 3 4 5 6 7 8 9 10 \\nNDCG@K \\nK\\nMovieLens \\nItemPop ItemKNN \\nBPR eALS \\nNeuMF (b) MovieLens — NDCG@K\\n0.1 \\n0.3 \\n0.5 \\n0.7 \\n0.9 \\n1 2 3 4 5 6 7 8 9 10 \\nHR@K \\nK\\nPinterest \\nItemPop \\nItemKNN \\nBPR \\neALS \\nNeuMF (c) Pinterest — HR@K\\n0.1 \\n0.22 \\n0.34 \\n0.46 \\n0.58 \\n1 2 3 4 5 6 7 8 9 10 \\nNDCG@K \\nK\\nPinterest \\nItemPop \\nItemKNN \\nBPR \\neALS \\nNeuMF (d) Pinterest — NDCG@K\\nFigure 5: Evaluation of Top- K item recommendation where K ranges from 1 to 10 on the two datasets.\\nowing to its pairwise ranking-aware learner. The neighbor-\\nbased ItemKNN underperforms model-based methods. And\\nItemPop performs the worst, indicating the necessity of mod-\\neling users’ personalized preferences, rather than just recom-\\nmending popular items to users.\\n4.2.1 Utility of Pre-training\\nTo demonstrate the utility of pre-training for NeuMF, we\\ncompared the performance of two versions of NeuMF —\\nwith and without pre-training. For NeuMF without pre-\\ntraining, we used the Adam to learn it with random ini-\\ntializations. As shown in Table 2, the NeuMF with pre-\\ntraining achieves better performance in most cases; only\\nfor MovieLens with a small predictive factors of 8, the pre-\\ntraining method performs slightly worse. The relative im-\\nprovements of the NeuMF with pre-training are 2 .2% and\\n1.1% for MovieLens and Pinterest, respectively. This re-\\nsult justiﬁes the usefulness of our pre-training method for\\ninitializing NeuMF.\\nTable 2: Performance of NeuMF with and without\\npre-training.\\nWith Pre-training Without Pre-training\\nFactors HR@10 NDCG@10 HR@10 NDCG@10\\nMovieLens\\n8 0.684 0.403 0.688 0.410\\n16 0.707 0.426 0.696 0.420\\n32 0.726 0.445 0.701 0.425\\n64 0.730 0.447 0.705 0.426\\nPinterest\\n8 0.878 0.555 0.869 0.546\\n16 0.880 0.558 0.871 0.547\\n32 0.879 0.555 0.870 0.549\\n64 0.877 0.552 0.872 0.551\\n4.3 Log Loss with Negative Sampling (RQ2)\\nTo deal with the one-class nature of implicit feedback,\\nwe cast recommendation as a binary classiﬁcation task. By\\nviewing NCF as a probabilistic model, we optimized it with\\nthe log loss. Figure 6 shows the training loss (averaged\\nover all instances) and recommendation performance of NCF\\nmethods of each iteration on MovieLens. Results on Pinter-\\nest show the same trend and thus they are omitted due to\\nspace limitation. First, we can see that with more iterations,\\nthe training loss of NCF models gradually decreases and\\nthe recommendation performance is improved. The most\\neﬀective updates are occurred in the ﬁrst 10 iterations, and\\nmore iterations may overﬁt a model (e.g., although the train-\\ning loss of NeuMF keeps decreasing after 10 iterations, its\\nrecommendation performance actually degrades). Second,\\namong the three NCF methods, NeuMF achieves the lowest\\ntraining loss, followed by MLP, and then GMF. The rec-\\nommendation performance also shows the same trend that\\nNeuMF >MLP >GMF. The above ﬁndings provide empir-\\nical evidence for the rationality and eﬀectiveness of optimiz-\\ning the log loss for learning from implicit data.\\nAn advantage of pointwise log loss over pairwise objective\\nfunctions [27, 33] is the ﬂexible sampling ratio for negative\\ninstances. While pairwise objective functions can pair only\\none sampled negative instance with a positive instance, we\\ncan ﬂexibly control the sampling ratio of a pointwise loss. To\\nillustrate the impact of negative sampling for NCF methods,\\nwe show the performance of NCF methods w.r.t. diﬀerent\\nnegative sampling ratios in Figure 7. It can be clearly seen\\nthat just one negative sample per positive instance is insuf-\\nﬁcient to achieve optimal performance, and sampling more\\nnegative instances is beneﬁcial. Comparing GMF to BPR,\\nwe can see the performance of GMF with a sampling ratio\\nof one is on par with BPR, while GMF signiﬁcantly betters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-08-29T01:23:59+00:00', 'author': '', 'keywords': '', 'moddate': '2017-08-29T01:23:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/NeuCF.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content='0.1 \\n0.2 \\n0.3 \\n0.4 \\n0.5 \\n0 10 20 30 40 50 \\nTraining Loss \\nIteration \\nMovieLens \\nGMF \\nMLP \\nNeuMF \\n(a) Training Loss\\n0.1 \\n0.3 \\n0.5 \\n0.7 \\n0 10 20 30 40 50 \\nHR@10 \\nIteration \\nMovieLens \\nGMF \\nMLP \\nNeuMF (b) HR@10\\n0\\n0.1 \\n0.2 \\n0.3 \\n0.4 \\n0.5 \\n0 10 20 30 40 50 \\nNDCG@10 \\nIteration \\nMovieLens \\nGMF \\nMLP \\nNeuMF (c) NDCG@10\\nFigure 6: Training loss and recommendation performance of NCF methods w.r.t. the number of iterations\\non MovieLens (factors=8).\\n0.62 \\n0.64 \\n0.66 \\n0.68 \\n0.7 \\n0.72 \\n1 2 3 4 5 6 7 8 9 10 \\nHR@10 \\nNumber of Negatives \\nMovieLens \\nNeuMF \\nGMF \\nMLP \\nBPR \\n(a) MovieLens — HR@10\\n0.36 \\n0.38 \\n0.4 \\n0.42 \\n0.44 \\n1 2 3 4 5 6 7 8 9 10 \\nNDCG@10 \\nNumber of Negatives \\nMovieLens \\nNeuMF \\nGMF \\nMLP \\nBPR (b) MovieLens — NDCG@10\\n0.84 \\n0.85 \\n0.86 \\n0.87 \\n0.88 \\n0.89 \\n1 2 3 4 5 6 7 8 9 10 \\nHR@10 \\nNumber of Negatives \\nPinterest \\nNeuMF \\nGMF \\nMLP \\nBPR (c) Pinterest — HR@10\\n0.52 \\n0.53 \\n0.54 \\n0.55 \\n0.56 \\n0.57 \\n1 2 3 4 5 6 7 8 9 10 \\nNDCG@10 \\nNumber of Negatives \\nPinterest \\nNeuMF GMF \\nMLP BPR (d) Pinterest — NDCG@10\\nFigure 7: Performance of NCF methods w.r.t. the number of negative samples per positive instance (fac-\\ntors=16). The performance of BPR is also shown, which samples only one negative instance to pair with a\\npositive instance for learning.\\nBPR with larger sampling ratios. This shows the advan-\\ntage of pointwise log loss over the pairwise BPR loss. For\\nboth datasets, the optimal sampling ratio is around 3 to 6.\\nOn Pinterest, we ﬁnd that when the sampling ratio is larger\\nthan 7, the performance of NCF methods starts to drop. It\\nreveals that setting the sampling ratio too aggressively may\\nadversely hurt the performance.\\n4.4 Is Deep Learning Helpful? (RQ3)\\nAs there is little work on learning user–item interaction\\nfunction with neural networks, it is curious to see whether\\nusing a deep network structure is beneﬁcial to the recom-\\nmendation task. Towards this end, we further investigated\\nMLP with diﬀerent number of hidden layers. The results\\nare summarized in Table 3 and 4. The MLP-3 indicates\\nthe MLP method with three hidden layers (besides the em-\\nbedding layer), and similar notations for others. As we can\\nsee, even for models with the same capability, stacking more\\nlayers are beneﬁcial to performance. This result is highly\\nencouraging, indicating the eﬀectiveness of using deep mod-\\nels for collaborative recommendation. We attribute the im-\\nprovement to the high non-linearities brought by stacking\\nmore non-linear layers. To verify this, we further tried stack-\\ning linear layers, using an identity function as the activation\\nfunction. The performance is much worse than using the\\nReLU unit.\\nFor MLP-0 that has no hidden layers (i.e., the embedding\\nlayer is directly projected to predictions), the performance is\\nvery weak and is not better than the non-personalized Item-\\nPop. This veriﬁes our argument in Section 3.3 that simply\\nconcatenating user and item latent vectors is insuﬃcient for\\nmodelling their feature interactions, and thus the necessity\\nof transforming it with hidden layers.\\nTable 3: HR@10 of MLP with diﬀerent layers.\\nFactors MLP-0 MLP-1 MLP-2 MLP-3 MLP-4\\nMovieLens\\n8 0.452 0.628 0.655 0.671 0.678\\n16 0.454 0.663 0.674 0.684 0.690\\n32 0.453 0.682 0.687 0.692 0.699\\n64 0.453 0.687 0.696 0.702 0.707\\nPinterest\\n8 0.275 0.848 0.855 0.859 0.862\\n16 0.274 0.855 0.861 0.865 0.867\\n32 0.273 0.861 0.863 0.868 0.867\\n64 0.274 0.864 0.867 0.869 0.873\\n5. RELATED WORK\\nWhile early literature on recommendation has largely fo-\\ncused on explicit feedback [30, 31], recent attention is in-\\ncreasingly shifting towards implicit data [1, 14, 23]. The\\ncollaborative ﬁltering (CF) task with implicit feedback is\\nusually formulated as an item recommendation problem, for\\nwhich the aim is to recommend a short list of items to users.\\nIn contrast to rating prediction that has been widely solved\\nby work on explicit feedback, addressing the item recommen-\\ndation problem is more practical but challenging [1, 11]. One\\nkey insight is to model the missing data, which are always\\nignored by the work on explicit feedback [21, 48]. To tailor\\nlatent factor models for item recommendation with implicit\\nfeedback, early work [19, 27] applies a uniform weighting\\nwhere two strategies have been proposed — which either\\ntreated all missing data as negative instances [19] or sam-\\npled negative instances from missing data [27]. Recently, He\\net al. [14] and Liang et al. [23] proposed dedicated models\\nto weight missing data, and Rendle et al. [1] developed an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-08-29T01:23:59+00:00', 'author': '', 'keywords': '', 'moddate': '2017-08-29T01:23:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/NeuCF.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9'}, page_content='Table 4: NDCG@10 of MLP with diﬀerent layers.\\nFactors MLP-0 MLP-1 MLP-2 MLP-3 MLP-4\\nMovieLens\\n8 0.253 0.359 0.383 0.399 0.406\\n16 0.252 0.391 0.402 0.410 0.415\\n32 0.252 0.406 0.410 0.425 0.423\\n64 0.251 0.409 0.417 0.426 0.432\\nPinterest\\n8 0.141 0.526 0.534 0.536 0.539\\n16 0.141 0.532 0.536 0.538 0.544\\n32 0.142 0.537 0.538 0.542 0.546\\n64 0.141 0.538 0.542 0.545 0.550\\nimplicit coordinate descent (iCD) solution for feature-based\\nfactorization models, achieving state-of-the-art performance\\nfor item recommendation. In the following, we discuss rec-\\nommendation works that use neural networks.\\nThe early pioneer work by Salakhutdinov et al. [30] pro-\\nposed a two-layer Restricted Boltzmann Machines (RBMs)\\nto model users’ explicit ratings on items. The work was been\\nlater extended to model the ordinal nature of ratings [36].\\nRecently, autoencoders have become a popular choice for\\nbuilding recommendation systems [32, 22, 35]. The idea of\\nuser-based AutoRec [32] is to learn hidden structures that\\ncan reconstruct a user’s ratings given her historical ratings\\nas inputs. In terms of user personalization, this approach\\nshares a similar spirit as the item–item model [31, 25] that\\nrepresents a user as her rated items. To avoid autoencoders\\nlearning an identity function and failing to generalize to un-\\nseen data, denoising autoencoders (DAEs) have been applied\\nto learn from intentionally corrupted inputs [22, 35]. More\\nrecently, Zheng et al. [48] presented a neural autoregressive\\nmethod for CF. While the previous eﬀort has lent support\\nto the eﬀectiveness of neural networks for addressing CF,\\nmost of them focused on explicit ratings and modelled the\\nobserved data only. As a result, they can easily fail to learn\\nusers’ preference from the positive-only implicit data.\\nAlthough some recent works [6, 37, 38, 43, 45] have ex-\\nplored deep learning models for recommendation based on\\nimplicit feedback, they primarily used DNNs for modelling\\nauxiliary information, such as textual description of items [38],\\nacoustic features of musics [37, 43], cross-domain behaviors\\nof users [6], and the rich information in knowledge bases [45].\\nThe features learnt by DNNs are then integrated with MF\\nfor CF. The work that is most relevant to our work is [44],\\nwhich presents a collaborative denoising autoencoder (CDAE)\\nfor CF with implicit feedback. In contrast to the DAE-based\\nCF [35], CDAE additionally plugs a user node to the input\\nof autoencoders for reconstructing the user’s ratings. As\\nshown by the authors, CDAE is equivalent to the SVD++\\nmodel [21] when the identity function is applied to acti-\\nvate the hidden layers of CDAE. This implies that although\\nCDAE is a neural modelling approach for CF, it still applies\\na linear kernel (i.e., inner product) to model user–item inter-\\nactions. This may partially explain why using deep layers for\\nCDAE does not improve the performance ( cf. Section 6 of\\n[44]). Distinct from CDAE, our NCF adopts a two-pathway\\narchitecture, modelling user–item interactions with a multi-\\nlayer feedforward neural network. This allows NCF to learn\\nan arbitrary function from the data, being more powerful\\nand expressive than the ﬁxed inner product function.\\nAlong a similar line, learning the relations of two enti-\\nties has been intensively studied in literature of knowledge\\ngraphs [2, 33]. Many relational machine learning methods\\nhave been devised [24]. The one that is most similar to our\\nproposal is the Neural Tensor Network (NTN) [33], which\\nuses neural networks to learn the interaction of two entities\\nand shows strong performance. Here we focus on a diﬀer-\\nent problem setting of CF. While the idea of NeuMF that\\ncombines MF with MLP is partially inspired by NTN, our\\nNeuMF is more ﬂexible and generic than NTN, in terms of\\nallowing MF and MLP learning diﬀerent sets of embeddings.\\nMore recently, Google publicized their Wide & Deep learn-\\ning approach for App recommendation [4]. The deep compo-\\nnent similarly uses a MLP on feature embeddings, which has\\nbeen reported to have strong generalization ability. While\\ntheir work has focused on incorporating various features\\nof users and items, we target at exploring DNNs for pure\\ncollaborative ﬁltering systems. We show that DNNs are a\\npromising choice for modelling user–item interactions, which\\nto our knowledge has not been investigated before.\\n6. CONCLUSION AND FUTURE WORK\\nIn this work, we explored neural network architectures\\nfor collaborative ﬁltering. We devised a general framework\\nNCF and proposed three instantiations — GMF, MLP and\\nNeuMF — that model user–item interactions in diﬀerent\\nways. Our framework is simple and generic; it is not limited\\nto the models presented in this paper, but is designed to\\nserve as a guideline for developing deep learning methods for\\nrecommendation. This work complements the mainstream\\nshallow models for collaborative ﬁltering, opening up a new\\navenue of research possibilities for recommendation based\\non deep learning.\\nIn future, we will study pairwise learners for NCF mod-\\nels and extend NCF to model auxiliary information, such\\nas user reviews [11], knowledge bases [45], and temporal sig-\\nnals [1]. While existing personalization models have primar-\\nily focused on individuals, it is interesting to develop models\\nfor groups of users, which help the decision-making for social\\ngroups [15, 42]. Moreover, we are particularly interested in\\nbuilding recommender systems for multi-media items, an in-\\nteresting task but has received relatively less scrutiny in the\\nrecommendation community [3]. Multi-media items, such as\\nimages and videos, contain much richer visual semantics [16,\\n41] that can reﬂect users’ interest. To build a multi-media\\nrecommender system, we need to develop eﬀective methods\\nto learn from multi-view and multi-modal data [13, 40]. An-\\nother emerging direction is to explore the potential of recur-\\nrent neural networks and hashing methods [46] for providing\\neﬃcient online recommendation [14, 1].\\nAcknowledgement\\nThe authors thank the anonymous reviewers for their valu-\\nable comments, which are beneﬁcial to the authors’ thoughts\\non recommendation systems and the revision of the paper.\\n7. REFERENCES\\n[1] I. Bayer, X. He, B. Kanagal, and S. Rendle. A generic\\ncoordinate descent framework for learning from implicit\\nfeedback. In WWW, 2017.\\n[2] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and\\nO. Yakhnenko. Translating embeddings for modeling\\nmulti-relational data. In NIPS, pages 2787–2795, 2013.\\n[3] T. Chen, X. He, and M.-Y. Kan. Context-aware image\\ntweet modelling and recommendation. In MM, pages\\n1018–1027, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-08-29T01:23:59+00:00', 'author': '', 'keywords': '', 'moddate': '2017-08-29T01:23:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/NeuCF.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10'}, page_content='[4] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra,\\nH. Aradhye, G. Anderson, G. Corrado, W. Chai, M. Ispir,\\net al. Wide & deep learning for recommender systems.\\narXiv preprint arXiv:1606.07792, 2016.\\n[5] R. Collobert and J. Weston. A uniﬁed architecture for\\nnatural language processing: Deep neural networks with\\nmultitask learning. In ICML, pages 160–167, 2008.\\n[6] A. M. Elkahky, Y. Song, and X. He. A multi-view deep\\nlearning approach for cross domain user modeling in\\nrecommendation systems. In WWW, pages 278–288, 2015.\\n[7] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol,\\nP. Vincent, and S. Bengio. Why does unsupervised\\npre-training help deep learning? Journal of Machine\\nLearning Research, 11:625–660, 2010.\\n[8] X. Geng, H. Zhang, J. Bian, and T.-S. Chua. Learning\\nimage and user features for recommendation in social\\nnetworks. In ICCV, pages 4274–4282, 2015.\\n[9] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer\\nneural networks. In AISTATS, pages 315–323, 2011.\\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual\\nlearning for image recognition. In CVPR, 2016.\\n[11] X. He, T. Chen, M.-Y. Kan, and X. Chen. TriRank:\\nReview-aware explainable recommendation by modeling\\naspects. In CIKM, pages 1661–1670, 2015.\\n[12] X. He, M. Gao, M.-Y. Kan, Y. Liu, and K. Sugiyama.\\nPredicting the popularity of web 2.0 items based on user\\ncomments. In SIGIR, pages 233–242, 2014.\\n[13] X. He, M.-Y. Kan, P. Xie, and X. Chen. Comment-based\\nmulti-view clustering of web 2.0 items. In WWW, pages\\n771–782, 2014.\\n[14] X. He, H. Zhang, M.-Y. Kan, and T.-S. Chua. Fast matrix\\nfactorization for online recommendation with implicit\\nfeedback. In SIGIR, pages 549–558, 2016.\\n[15] R. Hong, Z. Hu, L. Liu, M. Wang, S. Yan, and Q. Tian.\\nUnderstanding blooming human groups in social networks.\\nIEEE Transactions on Multimedia, 17(11):1980–1988, 2015.\\n[16] R. Hong, Y. Yang, M. Wang, and X. S. Hua. Learning\\nvisual semantic relationships for eﬃcient visual retrieval.\\nIEEE Transactions on Big Data, 1(4):152–161, 2015.\\n[17] K. Hornik, M. Stinchcombe, and H. White. Multilayer\\nfeedforward networks are universal approximators. Neural\\nNetworks, 2(5):359–366, 1989.\\n[18] L. Hu, A. Sun, and Y. Liu. Your neighbors aﬀect your\\nratings: On geographical neighborhood inﬂuence to rating\\nprediction. In SIGIR, pages 345–354, 2014.\\n[19] Y. Hu, Y. Koren, and C. Volinsky. Collaborative ﬁltering\\nfor implicit feedback datasets. In ICDM, pages 263–272,\\n2008.\\n[20] D. Kingma and J. Ba. Adam: A method for stochastic\\noptimization. In ICLR, pages 1–15, 2014.\\n[21] Y. Koren. Factorization meets the neighborhood: A\\nmultifaceted collaborative ﬁltering model. In KDD, pages\\n426–434, 2008.\\n[22] S. Li, J. Kawale, and Y. Fu. Deep collaborative ﬁltering via\\nmarginalized denoising auto-encoder. In CIKM, pages\\n811–820, 2015.\\n[23] D. Liang, L. Charlin, J. McInerney, and D. M. Blei.\\nModeling user exposure in recommendation. In WWW,\\npages 951–961, 2016.\\n[24] M. Nickel, K. Murphy, V. Tresp, and E. Gabrilovich. A\\nreview of relational machine learning for knowledge graphs.\\nProceedings of the IEEE, 104:11–33, 2016.\\n[25] X. Ning and G. Karypis. Slim: Sparse linear methods for\\ntop-n recommender systems. In ICDM, pages 497–506,\\n2011.\\n[26] S. Rendle. Factorization machines. In ICDM, pages\\n995–1000, 2010.\\n[27] S. Rendle, C. Freudenthaler, Z. Gantner, and\\nL. Schmidt-Thieme. Bpr: Bayesian personalized ranking\\nfrom implicit feedback. In UAI, pages 452–461, 2009.\\n[28] S. Rendle, Z. Gantner, C. Freudenthaler, and\\nL. Schmidt-Thieme. Fast context-aware recommendations\\nwith factorization machines. In SIGIR, pages 635–644,\\n2011.\\n[29] R. Salakhutdinov and A. Mnih. Probabilistic matrix\\nfactorization. In NIPS, pages 1–8, 2008.\\n[30] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted\\nboltzmann machines for collaborative ﬁltering. In ICDM,\\npages 791–798, 2007.\\n[31] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.\\nItem-based collaborative ﬁltering recommendation\\nalgorithms. In WWW, pages 285–295, 2001.\\n[32] S. Sedhain, A. K. Menon, S. Sanner, and L. Xie. Autorec:\\nAutoencoders meet collaborative ﬁltering. In WWW, pages\\n111–112, 2015.\\n[33] R. Socher, D. Chen, C. D. Manning, and A. Ng. Reasoning\\nwith neural tensor networks for knowledge base completion.\\nIn NIPS, pages 926–934, 2013.\\n[34] N. Srivastava and R. R. Salakhutdinov. Multimodal\\nlearning with deep boltzmann machines. In NIPS, pages\\n2222–2230, 2012.\\n[35] F. Strub and J. Mary. Collaborative ﬁltering with stacked\\ndenoising autoencoders and sparse inputs. In NIPS\\nWorkshop on Machine Learning for eCommerce, 2015.\\n[36] T. T. Truyen, D. Q. Phung, and S. Venkatesh. Ordinal\\nboltzmann machines for collaborative ﬁltering. In UAI,\\npages 548–556, 2009.\\n[37] A. Van den Oord, S. Dieleman, and B. Schrauwen. Deep\\ncontent-based music recommendation. In NIPS, pages\\n2643–2651, 2013.\\n[38] H. Wang, N. Wang, and D.-Y. Yeung. Collaborative deep\\nlearning for recommender systems. In KDD, pages\\n1235–1244, 2015.\\n[39] M. Wang, W. Fu, S. Hao, D. Tao, and X. Wu. Scalable\\nsemi-supervised learning by eﬃcient anchor graph\\nregularization. IEEE Transactions on Knowledge and Data\\nEngineering, 28(7):1864–1877, 2016.\\n[40] M. Wang, H. Li, D. Tao, K. Lu, and X. Wu. Multimodal\\ngraph-based reranking for web image search. IEEE\\nTransactions on Image Processing, 21(11):4649–4661, 2012.\\n[41] M. Wang, X. Liu, and X. Wu. Visual classiﬁcation by l1\\nhypergraph modeling. IEEE Transactions on Knowledge\\nand Data Engineering, 27(9):2564–2574, 2015.\\n[42] X. Wang, L. Nie, X. Song, D. Zhang, and T.-S. Chua.\\nUnifying virtual and physical worlds: Learning towards\\nlocal and global consistency. ACM Transactions on\\nInformation Systems, 2017.\\n[43] X. Wang and Y. Wang. Improving content-based and\\nhybrid music recommendation using deep learning. In MM,\\npages 627–636, 2014.\\n[44] Y. Wu, C. DuBois, A. X. Zheng, and M. Ester.\\nCollaborative denoising auto-encoders for top-n\\nrecommender systems. In WSDM, pages 153–162, 2016.\\n[45] F. Zhang, N. J. Yuan, D. Lian, X. Xie, and W.-Y. Ma.\\nCollaborative knowledge base embedding for recommender\\nsystems. In KDD, pages 353–362, 2016.\\n[46] H. Zhang, F. Shen, W. Liu, X. He, H. Luan, and T.-S.\\nChua. Discrete collaborative ﬁltering. In SIGIR, pages\\n325–334, 2016.\\n[47] H. Zhang, Y. Yang, H. Luan, S. Yang, and T.-S. Chua.\\nStart from scratch: Towards automatically identifying,\\nmodeling, and naming visual attributes. In MM, pages\\n187–196, 2014.\\n[48] Y. Zheng, B. Tang, W. Ding, and H. Zhou. A neural\\nautoregressive approach to collaborative ﬁltering. In ICML,\\npages 764–773, 2016.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load PDF document\n",
    "loader = PyPDFLoader(\"data/NeuCF.pdf\")\n",
    "pages = loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7269eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split document into much smaller trunk so that it is easy for model to consume and focus \n",
    "# need to adjust chunk size and overlap to handle different PDF format\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
    "                                               chunk_overlap = 200,\n",
    "                                                length_function=len,\n",
    "                                                 separators=['\\n\\n','\\n',' '] ) #seperator is page break, line break or space\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed5cc2e",
   "metadata": {},
   "source": [
    "#### <a id='Create Embeddings' style=\"color:blue;font-size:140%;\"> 4. Create Embeddings</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "206dd3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model = 'text-embedding-ada-002',\n",
    "        openai_api_key = OPENAI_API_KEY\n",
    "    )\n",
    "    return embeddings\n",
    "embedding_function = get_embedding_function()\n",
    "test_vector = embedding_function.embed_query(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4548e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.1402799252940825}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get intuition of using embedding to speed up retrieval\n",
    "evaluator = load_evaluator(evaluator=\"embedding_distance\",\n",
    "                           embeddings=embedding_function)\n",
    "evaluator.evaluate_strings(prediction='dog', reference='poodle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c077692b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between dog and poodle is {'score': 0.1402799252940825}\n",
      "The distance between cat and poodle is {'score': 0.2088812781695495}\n"
     ]
    }
   ],
   "source": [
    "#test the embedding to see if it is able to capture the difference between dog and poodle vs cat and poodle\n",
    "print(f'The distance between dog and poodle is {evaluator.evaluate_strings(prediction='dog', reference='poodle')}')\n",
    "print(f'The distance between cat and poodle is {evaluator.evaluate_strings(prediction='cat', reference='poodle')}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acdfe0a",
   "metadata": {},
   "source": [
    "#### <a id='Create Vector Database' style=\"color:blue;font-size:140%;\"> 5. Create Vector Database</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ed6ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a chroma database as a vector database to store all the embedding from the trunks\n",
    "#you need to specify the unique id for each document, otherwise, it is create duplicate vecors for the same file in chroma\n",
    "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
    "    #create a list of unique ids for each document based on the content\n",
    "    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]\n",
    "\n",
    "    # ensure that only unique docs with unique ids are kept\n",
    "    unique_ids = set()\n",
    "    unique_chunks = []\n",
    "\n",
    "    for chunk, id in zip(chunks, ids):\n",
    "        if id not in unique_ids:\n",
    "            unique_ids.add(id)\n",
    "            unique_chunks.append(chunk)\n",
    "\n",
    "    #create a chroma database as a vector database to store all the embedding from the trunks\n",
    "    vectorstore = Chroma.from_documents(documents=unique_chunks,\n",
    "                                        ids=list(unique_ids),\n",
    "                                        embedding=embedding_function,\n",
    "                                        persist_directory=vectorstore_path)\n",
    "\n",
    "    vectorstore.persist()\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3084d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vectorestore\n",
    "vectorstore = create_vectorstore(chunks=chunks,\n",
    "                                 embedding_function=embedding_function,\n",
    "                                 vectorstore_path=\"vectorstore_chroma_pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9400d1b9",
   "metadata": {},
   "source": [
    "#### <a id='Query Relevant information' style=\"color:blue;font-size:140%;\"> 6. Query Relevant information</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5af83783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vector store\n",
    "vectorstore = Chroma(persist_directory='vectorstore_chroma_pdf', embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a8578c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create retriever and get relevant chunks\n",
    "retriever = vectorstore.as_retriever(search_type='similarity') #use cosin similarity as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "add21778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'total_pages': 10, 'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-08-29T01:23:59+00:00', 'author': '', 'moddate': '2017-08-29T01:23:59+00:00', 'trapped': '/False', 'page': 1, 'subject': '', 'source': 'data/NeuCF.pdf', 'title': '', 'keywords': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'page_label': '2'}, page_content='can see, MF models the two-way interaction of user and item\\nlatent factors, assuming each dimension of the latent space\\nis independent of each other and linearly combining them\\nwith the same weight. As such, MF can be deemed as a\\nlinear model of latent factors.\\nFigure 1 illustrates how the inner product function can\\nlimit the expressiveness of MF. There are two settings to be\\nstated clearly beforehand to understand the example well.\\nFirst, since MF maps users and items to the same latent\\nspace, the similarity between two users can also be measured\\nwith an inner product, or equivalently 2, the cosine of the\\nangle between their latent vectors. Second, without loss of\\n2Assuming latent vectors are of a unit length.'),\n",
       " Document(metadata={'page_label': '1', 'moddate': '2017-08-29T01:23:59+00:00', 'creationdate': '2017-08-29T01:23:59+00:00', 'producer': 'pdfTeX-1.40.17', 'author': '', 'creator': 'LaTeX with hyperref package', 'title': '', 'source': 'data/NeuCF.pdf', 'keywords': '', 'subject': '', 'page': 0, 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'trapped': '/False', 'total_pages': 10}, page_content='play a pivotal role in alleviating information overload, hav-\\ning been widely adopted by many online services, including\\nE-commerce, online news and social media sites. The key to\\na personalized recommender system is in modelling users’\\npreference on items based on their past interactions ( e.g.,\\nratings and clicks), known as collaborative ﬁltering [31, 46].\\nAmong the various collaborative ﬁltering techniques, matrix\\nfactorization (MF) [14, 21] is the most popular one, which\\nprojects users and items into a shared latent space, using\\na vector of latent features to represent a user or an item.\\nThereafter a user’s interaction on an item is modelled as the\\ninner product of their latent vectors.\\nPopularized by the Netﬂix Prize, MF has become the de\\nfacto approach to latent factor model-based recommenda-\\ntion. Much research eﬀort has been devoted to enhancing\\nMF, such as integrating it with neighbor-based models [21],\\ncombining it with topic models of item content [38], and ex-\\ntending it to factorization machines [26] for a generic mod-\\nelling of features. Despite the eﬀectiveness of MF for collab-\\norative ﬁltering, it is well-known that its performance can be\\nhindered by the simple choice of the interaction function —\\ninner product. For example, for the task of rating prediction\\non explicit feedback, it is well known that the performance\\nof the MF model can be improved by incorporating user\\nand item bias terms into the interaction function 1. While'),\n",
       " Document(metadata={'title': '', 'total_pages': 10, 'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-08-29T01:23:59+00:00', 'page': 5, 'author': '', 'moddate': '2017-08-29T01:23:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'source': 'data/NeuCF.pdf', 'subject': '', 'keywords': '', 'page_label': '6', 'trapped': '/False'}, page_content='ularity. Since eALS shows superior performance over the\\nuniform-weighting method WMF [19], we do not further re-\\nport WMF’s performance.\\nAs our proposed methods aim to model the relationship\\nbetween users and items, we mainly compare with user–\\nitem models. We leave out the comparison with item–item\\nmodels, such as SLIM [25] and CDAE [44], because the per-\\nformance diﬀerence may be caused by the user models for\\npersonalization (as they are item–item model).\\nParameter Settings. We implemented our proposed meth-\\nods based on Keras 6. To determine hyper-parameters of\\nNCF methods, we randomly sampled one interaction for\\n6https://github.com/hexiangnan/neural_\\ncollaborative_filtering\\neach user as the validation data and tuned hyper-parameters\\non it. All NCF models are learnt by optimizing the log loss\\nof Equation 7, where we sampled four negative instances\\nper positive instance. For NCF models that are trained\\nfrom scratch, we randomly initialized model parameters with\\na Gaussian distribution (with a mean of 0 and standard\\ndeviation of 0 .01), optimizing the model with mini-batch\\nAdam [20]. We tested the batch size of [128 ,256,512,1024],\\nand the learning rate of [0.0001 ,0.0005, 0.001, 0.005]. Since\\nthe last hidden layer of NCF determines the model capa-\\nbility, we term it as predictive factors and evaluated the\\nfactors of [8,16,32,64]. It is worth noting that large factors\\nmay cause overﬁtting and degrade the performance. With-'),\n",
       " Document(metadata={'total_pages': 10, 'keywords': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'source': 'data/NeuCF.pdf', 'producer': 'pdfTeX-1.40.17', 'trapped': '/False', 'page_label': '6', 'title': '', 'creator': 'LaTeX with hyperref package', 'moddate': '2017-08-29T01:23:59+00:00', 'page': 5, 'creationdate': '2017-08-29T01:23:59+00:00', 'author': '', 'subject': ''}, page_content='forms that of eALS and BPR with a large factor of 64. This\\nindicates the high expressiveness of NeuMF by fusing the\\nlinear MF and non-linear MLP models. Second, the other\\ntwo NCF methods — GMF and MLP — also show quite\\nstrong performance. Between them, MLP slightly under-\\nperforms GMF. Note that MLP can be further improved by\\nadding more hidden layers (see Section 4.4), and here we\\nonly show the performance of three layers. For small pre-\\ndictive factors, GMF outperforms eALS on both datasets;\\nalthough GMF suﬀers from overﬁtting for large factors, its\\nbest performance obtained is better than (or on par with)\\nthat of eALS. Lastly, GMF shows consistent improvements\\nover BPR, admitting the eﬀectiveness of the classiﬁcation-\\naware log loss for the recommendation task, since GMF and\\nBPR learn the same MF model but with diﬀerent objective\\nfunctions.\\nFigure 5 shows the performance of Top- K recommended\\nlists where the ranking position K ranges from 1 to 10. To\\nmake the ﬁgure more clear, we show the performance of\\nNeuMF rather than all three NCF methods. As can be\\nseen, NeuMF demonstrates consistent improvements over\\nother methods across positions, and we further conducted\\none-sample paired t-tests, verifying that all improvements\\nare statistically signiﬁcant for p< 0.01. For baseline meth-\\nods, eALS outperforms BPR on MovieLens with about 5.1%\\nrelative improvement, while underperforms BPR on Pinter-\\nest in terms of NDCG. This is consistent with [14]’s ﬁnding')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test out the relevant chunks finding\n",
    "relevant_chunks = retriever.invoke(\"what's the advantages of this model?\")\n",
    "relevant_chunks\n",
    "#in the example below, it can really shows the difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88d539",
   "metadata": {},
   "source": [
    "#### <a id='Define Prompt Template' style=\"color:blue;font-size:140%;\"> 7. Define Prompt Template</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d76f4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt template\n",
    "#context is the information from retriever, and question is the actual question\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a research assistant for question-answering task.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, say that you don't know.\n",
    "DON'T MAKE UP ANYTHING YOU ARE NOT SURE ABOUT.\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2c0de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "You are a research assistant for question-answering task.\n",
      "Use the following pieces of retrieved context to answer the question.\n",
      "If you don't know the answer, say that you don't know.\n",
      "DON'T MAKE UP ANYTHING YOU ARE NOT SURE ABOUT.\n",
      "\n",
      "can see, MF models the two-way interaction of user and item\n",
      "latent factors, assuming each dimension of the latent space\n",
      "is independent of each other and linearly combining them\n",
      "with the same weight. As such, MF can be deemed as a\n",
      "linear model of latent factors.\n",
      "Figure 1 illustrates how the inner product function can\n",
      "limit the expressiveness of MF. There are two settings to be\n",
      "stated clearly beforehand to understand the example well.\n",
      "First, since MF maps users and items to the same latent\n",
      "space, the similarity between two users can also be measured\n",
      "with an inner product, or equivalently 2, the cosine of the\n",
      "angle between their latent vectors. Second, without loss of\n",
      "2Assuming latent vectors are of a unit length.\n",
      "\n",
      "---\n",
      "\n",
      "play a pivotal role in alleviating information overload, hav-\n",
      "ing been widely adopted by many online services, including\n",
      "E-commerce, online news and social media sites. The key to\n",
      "a personalized recommender system is in modelling users’\n",
      "preference on items based on their past interactions ( e.g.,\n",
      "ratings and clicks), known as collaborative ﬁltering [31, 46].\n",
      "Among the various collaborative ﬁltering techniques, matrix\n",
      "factorization (MF) [14, 21] is the most popular one, which\n",
      "projects users and items into a shared latent space, using\n",
      "a vector of latent features to represent a user or an item.\n",
      "Thereafter a user’s interaction on an item is modelled as the\n",
      "inner product of their latent vectors.\n",
      "Popularized by the Netﬂix Prize, MF has become the de\n",
      "facto approach to latent factor model-based recommenda-\n",
      "tion. Much research eﬀort has been devoted to enhancing\n",
      "MF, such as integrating it with neighbor-based models [21],\n",
      "combining it with topic models of item content [38], and ex-\n",
      "tending it to factorization machines [26] for a generic mod-\n",
      "elling of features. Despite the eﬀectiveness of MF for collab-\n",
      "orative ﬁltering, it is well-known that its performance can be\n",
      "hindered by the simple choice of the interaction function —\n",
      "inner product. For example, for the task of rating prediction\n",
      "on explicit feedback, it is well known that the performance\n",
      "of the MF model can be improved by incorporating user\n",
      "and item bias terms into the interaction function 1. While\n",
      "\n",
      "---\n",
      "\n",
      "ularity. Since eALS shows superior performance over the\n",
      "uniform-weighting method WMF [19], we do not further re-\n",
      "port WMF’s performance.\n",
      "As our proposed methods aim to model the relationship\n",
      "between users and items, we mainly compare with user–\n",
      "item models. We leave out the comparison with item–item\n",
      "models, such as SLIM [25] and CDAE [44], because the per-\n",
      "formance diﬀerence may be caused by the user models for\n",
      "personalization (as they are item–item model).\n",
      "Parameter Settings. We implemented our proposed meth-\n",
      "ods based on Keras 6. To determine hyper-parameters of\n",
      "NCF methods, we randomly sampled one interaction for\n",
      "6https://github.com/hexiangnan/neural_\n",
      "collaborative_filtering\n",
      "each user as the validation data and tuned hyper-parameters\n",
      "on it. All NCF models are learnt by optimizing the log loss\n",
      "of Equation 7, where we sampled four negative instances\n",
      "per positive instance. For NCF models that are trained\n",
      "from scratch, we randomly initialized model parameters with\n",
      "a Gaussian distribution (with a mean of 0 and standard\n",
      "deviation of 0 .01), optimizing the model with mini-batch\n",
      "Adam [20]. We tested the batch size of [128 ,256,512,1024],\n",
      "and the learning rate of [0.0001 ,0.0005, 0.001, 0.005]. Since\n",
      "the last hidden layer of NCF determines the model capa-\n",
      "bility, we term it as predictive factors and evaluated the\n",
      "factors of [8,16,32,64]. It is worth noting that large factors\n",
      "may cause overﬁtting and degrade the performance. With-\n",
      "\n",
      "---\n",
      "\n",
      "forms that of eALS and BPR with a large factor of 64. This\n",
      "indicates the high expressiveness of NeuMF by fusing the\n",
      "linear MF and non-linear MLP models. Second, the other\n",
      "two NCF methods — GMF and MLP — also show quite\n",
      "strong performance. Between them, MLP slightly under-\n",
      "performs GMF. Note that MLP can be further improved by\n",
      "adding more hidden layers (see Section 4.4), and here we\n",
      "only show the performance of three layers. For small pre-\n",
      "dictive factors, GMF outperforms eALS on both datasets;\n",
      "although GMF suﬀers from overﬁtting for large factors, its\n",
      "best performance obtained is better than (or on par with)\n",
      "that of eALS. Lastly, GMF shows consistent improvements\n",
      "over BPR, admitting the eﬀectiveness of the classiﬁcation-\n",
      "aware log loss for the recommendation task, since GMF and\n",
      "BPR learn the same MF model but with diﬀerent objective\n",
      "functions.\n",
      "Figure 5 shows the performance of Top- K recommended\n",
      "lists where the ranking position K ranges from 1 to 10. To\n",
      "make the ﬁgure more clear, we show the performance of\n",
      "NeuMF rather than all three NCF methods. As can be\n",
      "seen, NeuMF demonstrates consistent improvements over\n",
      "other methods across positions, and we further conducted\n",
      "one-sample paired t-tests, verifying that all improvements\n",
      "are statistically signiﬁcant for p< 0.01. For baseline meth-\n",
      "ods, eALS outperforms BPR on MovieLens with about 5.1%\n",
      "relative improvement, while underperforms BPR on Pinter-\n",
      "est in terms of NDCG. This is consistent with [14]’s ﬁnding\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What is the title of the paper?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate context text\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Create prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, \n",
    "                                question=\"What is the title of the paper?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277701ce",
   "metadata": {},
   "source": [
    "#### <a id='Define Langchain' style=\"color:blue;font-size:140%;\"> 8. Define Langchain</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3563e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "             |prompt_template\n",
    "             |llm\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7cf4a0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The advantages of the model, particularly the NeuMF (Neural Matrix Factorization) model, include:\\n\\n1. **High Expressiveness**: NeuMF combines both linear matrix factorization (MF) and non-linear multilayer perceptron (MLP) models, allowing for improved modeling of relationships between users and items.\\n\\n2. **Improved Performance**: NeuMF demonstrates consistent improvements in performance over other methods across different ranking positions, showcasing its efficacy in generating recommendations.\\n\\n3. **Statistically Significant Improvements**: The performance enhancements provided by NeuMF have been verified to be statistically significant, indicating strong reliability in its effectiveness.\\n\\n4. **Overfitting Management**: While GMF shows strong performance, the report notes that NeuMF is less prone to overfitting due to its advanced architecture, allowing for better generalization across datasets.\\n\\n5. **Robust Against Different Factors**: NeuMF maintains robust performance even as predictive factors are adjusted, highlighting its versatility compared to traditional models.\\n\\nOverall, the model leverages a sophisticated architecture that leads to better capturing of user-item interactions, thereby enhancing the recommendation system's accuracy.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 227, 'prompt_tokens': 1300, 'total_tokens': 1527, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BPh8RTwPcMVtoRUpBTkVWLBN54Mvn', 'finish_reason': 'stop', 'logprobs': None}, id='run-d78b214a-0c40-4669-bdff-acfaaa38498c-0', usage_metadata={'input_tokens': 1300, 'output_tokens': 227, 'total_tokens': 1527, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"what's the advantages of this model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d51c74",
   "metadata": {},
   "source": [
    "#### <a id='Structure Response' style=\"color:blue;font-size:140%;\"> 9. Structure Response</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d344b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithSources(BaseModel):\n",
    "    \"\"\"\n",
    "    An answer to the question, with sources and reasoning.\n",
    "    \"\"\"\n",
    "    answer: str = Field(description=\"Answer to the question\")\n",
    "    sources: str = Field(description=\"Full direct text chunk from the context used to answer the question\")\n",
    "    reasoning: str = Field(description=\"Explain the reasoning of the answer based on sources\")\n",
    "\n",
    "class ExtractedInfo(BaseModel):\n",
    "    \"\"\"Extracted information about the research article\"\"\"\n",
    "    paper_summary: AnswerWithSources\n",
    "    model_motivation: AnswerWithSources\n",
    "    model_architecture: AnswerWithSources\n",
    "    model_limitation: AnswerWithSources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07ed6e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuoeryang/Documents/rag_exp/rag_pdf/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1660: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nquestions = [\\n    \"Give me the title, summary, publication date, authors of the research paper.\",\\n    \"Give me the experiment dataset, baseline and evaluation metrics of the research paper.\",\\n    \"Give me the model motivation, architecture, and limitations.\"\\n]\\nresults = rag_chain.batch(questions) \\nfor q, info in zip(questions, results):\\n    print(\"▶ QUESTION:\", q)\\n    print(\"▶ ANSWER:\", info)\\n    print()\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm.with_structured_output(ExtractedInfo, strict=True)\n",
    "        )\n",
    "\n",
    "rag_chain.invoke(\"Give me the paper summary, model motivation, model architecture, and model limitation of the research paper.\")\n",
    "\n",
    "#below is the example of multiple prompt and answers at one shot\n",
    "\"\"\"\n",
    "questions = [\n",
    "    \"Give me the title, summary, publication date, authors of the research paper.\",\n",
    "    \"Give me the experiment dataset, baseline and evaluation metrics of the research paper.\",\n",
    "    \"Give me the model motivation, architecture, and limitations.\"\n",
    "]\n",
    "results = rag_chain.batch(questions) \n",
    "for q, info in zip(questions, results):\n",
    "    print(\"▶ QUESTION:\", q)\n",
    "    print(\"▶ ANSWER:\", info)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da6e9c",
   "metadata": {},
   "source": [
    "#### <a id='Structure Response' style=\"color:blue;font-size:140%;\"> 10. Structure Response into Tabular Format for User Consumption</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba1e8b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_summary</th>\n",
       "      <th>model_motivation</th>\n",
       "      <th>model_architecture</th>\n",
       "      <th>model_limitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td>The research paper discusses matrix factorizat...</td>\n",
       "      <td>The motivation behind the model is to enhance ...</td>\n",
       "      <td>The architecture involves projecting users and...</td>\n",
       "      <td>A key limitation of the proposed model is its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>MF models the two-way interaction of user and ...</td>\n",
       "      <td>Despite the effectiveness of MF for collaborat...</td>\n",
       "      <td>Popularized by the Netflix Prize, MF has becom...</td>\n",
       "      <td>It's worth noting that large factors may cause...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasoning</th>\n",
       "      <td>The text provides a clear description of MF an...</td>\n",
       "      <td>The motivation is rooted in the realization th...</td>\n",
       "      <td>The architecture description combines both lin...</td>\n",
       "      <td>The limitation is directly noted in the text, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paper_summary  \\\n",
       "answer     The research paper discusses matrix factorizat...   \n",
       "source     MF models the two-way interaction of user and ...   \n",
       "reasoning  The text provides a clear description of MF an...   \n",
       "\n",
       "                                            model_motivation  \\\n",
       "answer     The motivation behind the model is to enhance ...   \n",
       "source     Despite the effectiveness of MF for collaborat...   \n",
       "reasoning  The motivation is rooted in the realization th...   \n",
       "\n",
       "                                          model_architecture  \\\n",
       "answer     The architecture involves projecting users and...   \n",
       "source     Popularized by the Netflix Prize, MF has becom...   \n",
       "reasoning  The architecture description combines both lin...   \n",
       "\n",
       "                                            model_limitation  \n",
       "answer     A key limitation of the proposed model is its ...  \n",
       "source     It's worth noting that large factors may cause...  \n",
       "reasoning  The limitation is directly noted in the text, ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_response = rag_chain.invoke(\"Give me the paper summary, model motivation, model architecture, and model limitation of the research paper.\")\n",
    "df = pd.DataFrame([structured_response.dict()])\n",
    "\n",
    "# Transforming into a table with two rows: 'answer' and 'source'\n",
    "answer_row = []\n",
    "source_row = []\n",
    "reasoning_row = []\n",
    "\n",
    "for col in df.columns:\n",
    "    answer_row.append(df[col][0]['answer'])\n",
    "    source_row.append(df[col][0]['sources'])\n",
    "    reasoning_row.append(df[col][0]['reasoning'])\n",
    "\n",
    "# Create new dataframe with two rows: 'answer' and 'source'\n",
    "structured_response_df = pd.DataFrame([answer_row, source_row, reasoning_row], columns=df.columns, index=['answer', 'source', 'reasoning'])\n",
    "structured_response_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_pdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
